{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c86deb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exists: wa_od_main_JT00_2022.csv.gz\n",
      "✅ Exists: wa_xwalk.csv.gz\n",
      "✅ Exists: tl_2021_53_tract\n",
      "✅ Exists: tl_2021_53_bg\n",
      "✅ Exists: tl_2021_53_place\n",
      "▶ OD rows: 2,905,296\n",
      "▶ Unique work tracts: 1,771 | work BGs: 5,292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lette\\AppData\\Local\\Temp\\ipykernel_618512\\2803506025.py:89: DtypeWarning: Columns (31,35) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  xw = pd.read_csv(xw_fp, dtype={\"tabblk2020\":\"string\"})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Seattle polygon loaded\n",
      "▶ BGs intersecting Seattle: 571\n",
      "▶ Tracts intersecting Seattle: 199\n",
      "▶ Block-level flows (Seattle dest): 544,781\n",
      "▶ OD rows with BG-work in Seattle: 563,744\n",
      "▶ BG→BG pairs: 281,474\n",
      "▶ Block-group flows (final): 281,474\n",
      "▶ OD rows with tract-work in Seattle: 578,207\n",
      "▶ Tract→Tract pairs: 122,950\n",
      "▶ Tract flows (final): 122,950\n",
      "✅ Saved:\n",
      " - data/seattle_flows_block_2022.csv\n",
      " - data/seattle_flows_blockgroup_2022.csv\n",
      " - data/seattle_flows_tract_2022.csv\n",
      " - data/seattle_flows_multilevel_2022.csv  (all levels in one file)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Seattle Commuting Flows (LODES8, WA 2022)\n",
    "# - Block, Block Group, and Tract levels\n",
    "# - GEOIDs derived directly from 15-digit block codes (2020-compatible)\n",
    "# - Destinations filtered to polygons that INTERSECT Seattle\n",
    "# - Centroids computed in projected CRS; diagnostics printed\n",
    "# - Exports: block / blockgroup / tract CSVs + combined CSV\n",
    "# ==========================================================\n",
    "\n",
    "import os, io, zipfile, requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# -----------------------------\n",
    "# Config & helpers\n",
    "# -----------------------------\n",
    "DATA = Path(\"data\"); DATA.mkdir(exist_ok=True)\n",
    "\n",
    "def fetch(url: str, out_path: Path):\n",
    "    if out_path.exists():\n",
    "        print(f\"✅ Exists: {out_path.name}\")\n",
    "        return out_path\n",
    "    print(f\"⬇️  Downloading: {url}\")\n",
    "    r = requests.get(url, timeout=180)\n",
    "    r.raise_for_status()\n",
    "    out_path.write_bytes(r.content)\n",
    "    print(f\"✅ Saved: {out_path.name}\")\n",
    "    return out_path\n",
    "\n",
    "def fetch_unzip(url: str, out_dir: Path) -> Path:\n",
    "    out_dir.mkdir(exist_ok=True, parents=True)\n",
    "    shp = list(out_dir.glob(\"*.shp\"))\n",
    "    if shp:\n",
    "        print(f\"✅ Exists: {out_dir.name}\")\n",
    "        return shp[0]\n",
    "    print(f\"⬇️  Downloading & unzipping: {url}\")\n",
    "    r = requests.get(url, timeout=240)\n",
    "    r.raise_for_status()\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
    "        z.extractall(out_dir)\n",
    "    shp = list(out_dir.glob(\"*.shp\"))\n",
    "    print(f\"✅ Unzipped: {out_dir} ({shp[0].name})\")\n",
    "    return shp[0]\n",
    "\n",
    "def geounion(gdf: gpd.GeoDataFrame):\n",
    "    # union_all for newer versions; fallback to unary_union\n",
    "    return gdf.union_all() if hasattr(gdf, \"union_all\") else gdf.unary_union\n",
    "\n",
    "def centroid_lonlat(gdf: gpd.GeoDataFrame, proj_epsg=3857):\n",
    "    g_proj = gdf.to_crs(proj_epsg)\n",
    "    cents = g_proj.geometry.centroid\n",
    "    cents_ll = gpd.GeoSeries(cents, crs=g_proj.crs).to_crs(4326)\n",
    "    return pd.DataFrame({\"lon\": cents_ll.x.values, \"lat\": cents_ll.y.values})\n",
    "\n",
    "def log(msg): print(f\"▶ {msg}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Download inputs\n",
    "# -----------------------------\n",
    "od_fp  = fetch(\"https://lehd.ces.census.gov/data/lodes/LODES8/wa/od/wa_od_main_JT00_2022.csv.gz\",\n",
    "               DATA / \"wa_od_main_JT00_2022.csv.gz\")\n",
    "xw_fp  = fetch(\"https://lehd.ces.census.gov/data/lodes/LODES8/wa/wa_xwalk.csv.gz\",\n",
    "               DATA / \"wa_xwalk.csv.gz\")\n",
    "tracts_shp  = fetch_unzip(\"https://www2.census.gov/geo/tiger/TIGER2021/TRACT/tl_2021_53_tract.zip\",\n",
    "                          DATA / \"tl_2021_53_tract\")\n",
    "bg_shp      = fetch_unzip(\"https://www2.census.gov/geo/tiger/TIGER2021/BG/tl_2021_53_bg.zip\",\n",
    "                          DATA / \"tl_2021_53_bg\")\n",
    "places_shp  = fetch_unzip(\"https://www2.census.gov/geo/tiger/TIGER2021/PLACE/tl_2021_53_place.zip\",\n",
    "                          DATA / \"tl_2021_53_place\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Load OD and derive GEOIDs from block codes (2020-compatible)\n",
    "# -----------------------------\n",
    "od = pd.read_csv(od_fp, dtype={\"h_geocode\":\"string\",\"w_geocode\":\"string\"})\n",
    "od[\"S000\"] = od[\"S000\"].astype(\"int64\")\n",
    "log(f\"OD rows: {len(od):,}\")\n",
    "\n",
    "# Ensure 15 digits; derive tract (11) and block group (12) from block IDs\n",
    "od[\"h_geocode\"] = od[\"h_geocode\"].str.zfill(15)\n",
    "od[\"w_geocode\"] = od[\"w_geocode\"].str.zfill(15)\n",
    "od[\"tract_home\"] = od[\"h_geocode\"].str[:11]\n",
    "od[\"tract_work\"] = od[\"w_geocode\"].str[:11]\n",
    "od[\"bg_home\"]    = od[\"h_geocode\"].str[:12]\n",
    "od[\"bg_work\"]    = od[\"w_geocode\"].str[:12]\n",
    "log(f\"Unique work tracts: {od['tract_work'].nunique():,} | work BGs: {od['bg_work'].nunique():,}\")\n",
    "\n",
    "# For block-level coordinates we can use crosswalk block coords (lat/lon)\n",
    "xw = pd.read_csv(xw_fp, dtype={\"tabblk2020\":\"string\"})\n",
    "xw[\"tabblk2020\"] = xw[\"tabblk2020\"].str.zfill(15)\n",
    "blk_coords = xw.rename(columns={\"tabblk2020\":\"geocode\",\n",
    "                                \"blklondd\":\"lon\",\n",
    "                                \"blklatdd\":\"lat\"})[[\"geocode\",\"lon\",\"lat\"]]\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Load TIGER shapes (WGS84) + Seattle polygon\n",
    "# -----------------------------\n",
    "tracts = gpd.read_file(tracts_shp).to_crs(4326)\n",
    "tracts[\"GEOID\"] = tracts[\"GEOID\"].astype(str).str.zfill(11)\n",
    "bg = gpd.read_file(bg_shp).to_crs(4326)\n",
    "bg[\"GEOID\"] = bg[\"GEOID\"].astype(str).str.zfill(12)\n",
    "\n",
    "places = gpd.read_file(places_shp).to_crs(4326)\n",
    "seattle = places.query(\"NAME == 'Seattle'\").copy()\n",
    "assert len(seattle) == 1, \"Seattle boundary not found or multiple matches.\"\n",
    "SEA = geounion(seattle)\n",
    "log(\"Seattle polygon loaded\")\n",
    "\n",
    "# Precompute accurate centroids for BG & tracts (projected → back to lon/lat)\n",
    "bg_cent = centroid_lonlat(bg)\n",
    "bg = pd.concat([bg.reset_index(drop=True), bg_cent], axis=1)     # adds lon/lat\n",
    "tracts_cent = centroid_lonlat(tracts)\n",
    "tracts = pd.concat([tracts.reset_index(drop=True), tracts_cent], axis=1)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Build ID sets of destinations that touch Seattle (intersects)\n",
    "# -----------------------------\n",
    "# Do spatial tests in a metric CRS for robustness\n",
    "bg_m = bg.to_crs(3857); tracts_m = tracts.to_crs(3857); seattle_m = seattle.to_crs(3857)\n",
    "SEA_M = geounion(seattle_m)\n",
    "\n",
    "bg_ids_in_sea  = set(bg_m[bg_m.intersects(SEA_M)].index)\n",
    "tr_ids_in_sea  = set(tracts_m[tracts_m.intersects(SEA_M)].index)\n",
    "bg_geoids_in_sea = set(bg.iloc[list(bg_ids_in_sea)][\"GEOID\"]) if bg_ids_in_sea else set()\n",
    "tr_geoids_in_sea = set(tracts.iloc[list(tr_ids_in_sea)][\"GEOID\"]) if tr_ids_in_sea else set()\n",
    "log(f\"BGs intersecting Seattle: {len(bg_geoids_in_sea):,}\")\n",
    "log(f\"Tracts intersecting Seattle: {len(tr_geoids_in_sea):,}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) BLOCK → BLOCK flows (coords from crosswalk; polygon filter)\n",
    "# -----------------------------\n",
    "flows_block = od[[\"h_geocode\",\"w_geocode\",\"S000\"]].rename(columns={\"S000\":\"jobs\"})\n",
    "flows_block = flows_block.merge(blk_coords, left_on=\"h_geocode\", right_on=\"geocode\", how=\"left\") \\\n",
    "                         .rename(columns={\"lon\":\"home_lon\",\"lat\":\"home_lat\"}).drop(columns=[\"geocode\"])\n",
    "flows_block = flows_block.merge(blk_coords, left_on=\"w_geocode\", right_on=\"geocode\", how=\"left\") \\\n",
    "                         .rename(columns={\"lon\":\"work_lon\",\"lat\":\"work_lat\"}).drop(columns=[\"geocode\"])\n",
    "\n",
    "flows_block_gdf = gpd.GeoDataFrame(\n",
    "    flows_block,\n",
    "    geometry=gpd.points_from_xy(flows_block[\"work_lon\"], flows_block[\"work_lat\"]),\n",
    "    crs=4326\n",
    ").to_crs(3857)\n",
    "\n",
    "flows_block_sea = flows_block_gdf[flows_block_gdf.intersects(SEA_M)] \\\n",
    "                      .dropna(subset=[\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\"]).copy()\n",
    "flows_block_sea[\"geo_level\"] = \"block\"\n",
    "log(f\"Block-level flows (Seattle dest): {len(flows_block_sea):,}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) BLOCK GROUP → BLOCK GROUP flows (ID-based dest filter)\n",
    "# -----------------------------\n",
    "# Filter by destination BG GEOIDs that touch Seattle, then aggregate\n",
    "od_bg = od[od[\"bg_work\"].isin(bg_geoids_in_sea)].copy()\n",
    "log(f\"OD rows with BG-work in Seattle: {len(od_bg):,}\")\n",
    "\n",
    "flows_bg = (od_bg.groupby([\"bg_home\",\"bg_work\"], as_index=False)[\"S000\"]\n",
    "                 .sum().rename(columns={\"S000\":\"jobs\"}))\n",
    "log(f\"BG→BG pairs: {len(flows_bg):,}\")\n",
    "\n",
    "# Attach BG centroids\n",
    "bg_pts = bg[[\"GEOID\",\"lon\",\"lat\"]].copy()\n",
    "flows_bg = flows_bg.merge(bg_pts, left_on=\"bg_home\", right_on=\"GEOID\", how=\"left\") \\\n",
    "                   .rename(columns={\"lon\":\"home_lon\",\"lat\":\"home_lat\"}).drop(columns=[\"GEOID\"])\n",
    "flows_bg = flows_bg.merge(bg_pts, left_on=\"bg_work\", right_on=\"GEOID\", how=\"left\") \\\n",
    "                   .rename(columns={\"lon\":\"work_lon\",\"lat\":\"work_lat\"}).drop(columns=[\"GEOID\"])\n",
    "flows_bg = flows_bg.dropna(subset=[\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\"]).copy()\n",
    "flows_bg[\"geo_level\"] = \"blockgroup\"\n",
    "log(f\"Block-group flows (final): {len(flows_bg):,}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7) TRACT → TRACT flows (ID-based dest filter)\n",
    "# -----------------------------\n",
    "od_tr = od[od[\"tract_work\"].isin(tr_geoids_in_sea)].copy()\n",
    "log(f\"OD rows with tract-work in Seattle: {len(od_tr):,}\")\n",
    "\n",
    "flows_tr = (od_tr.groupby([\"tract_home\",\"tract_work\"], as_index=False)[\"S000\"]\n",
    "                 .sum().rename(columns={\"S000\":\"jobs\"}))\n",
    "log(f\"Tract→Tract pairs: {len(flows_tr):,}\")\n",
    "\n",
    "# Attach tract centroids\n",
    "tr_pts = tracts[[\"GEOID\",\"lon\",\"lat\"]].copy()\n",
    "flows_tr = flows_tr.merge(tr_pts, left_on=\"tract_home\", right_on=\"GEOID\", how=\"left\") \\\n",
    "                   .rename(columns={\"lon\":\"home_lon\",\"lat\":\"home_lat\"}).drop(columns=[\"GEOID\"])\n",
    "flows_tr = flows_tr.merge(tr_pts, left_on=\"tract_work\", right_on=\"GEOID\", how=\"left\") \\\n",
    "                   .rename(columns={\"lon\":\"work_lon\",\"lat\":\"work_lat\"}).drop(columns=[\"GEOID\"])\n",
    "flows_tr = flows_tr.dropna(subset=[\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\"]).copy()\n",
    "flows_tr[\"geo_level\"] = \"tract\"\n",
    "log(f\"Tract flows (final): {len(flows_tr):,}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Save outputs (and combined)\n",
    "# -----------------------------\n",
    "cols = [\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\",\"jobs\",\"geo_level\"]\n",
    "\n",
    "flows_block_sea[cols].to_csv(DATA/\"seattle_flows_block_2022.csv\", index=False)\n",
    "flows_bg[cols].to_csv(DATA/\"seattle_flows_blockgroup_2022.csv\", index=False)\n",
    "flows_tr[cols].to_csv(DATA/\"seattle_flows_tract_2022.csv\", index=False)\n",
    "\n",
    "flows_all = pd.concat([flows_block_sea[cols], flows_bg[cols], flows_tr[cols]], ignore_index=True)\n",
    "flows_all.to_csv(DATA/\"seattle_flows_multilevel_2022.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved:\")\n",
    "print(\" - data/seattle_flows_block_2022.csv\")\n",
    "print(\" - data/seattle_flows_blockgroup_2022.csv\")\n",
    "print(\" - data/seattle_flows_tract_2022.csv\")\n",
    "print(\" - data/seattle_flows_multilevel_2022.csv  (all levels in one file)\")\n",
    "\n",
    "# -----------------------------\n",
    "# 9) (Optional) Thin tiny flows for Kepler performance\n",
    "# -----------------------------\n",
    "# flows_all_min20 = flows_all[flows_all[\"jobs\"] >= 20].copy()\n",
    "# flows_all_min20.to_csv(DATA/\"seattle_flows_multilevel_2022_min20.csv\", index=False)\n",
    "# print(\"✅ Also saved: data/seattle_flows_multilevel_2022_min20.csv (jobs >= 20)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "momepy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
