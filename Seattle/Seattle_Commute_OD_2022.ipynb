{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a9e233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️  Downloading: https://lehd.ces.census.gov/data/lodes/LODES8/wa/od/wa_od_main_JT00_2022.csv.gz\n",
      "✅ Saved: wa_od_main_JT00_2022.csv.gz\n",
      "⬇️  Downloading: https://lehd.ces.census.gov/data/lodes/LODES8/wa/wa_xwalk.csv.gz\n",
      "✅ Saved: wa_xwalk.csv.gz\n",
      "⬇️  Downloading & unzipping: https://www2.census.gov/geo/tiger/TIGER2021/TRACT/tl_2021_53_tract.zip\n",
      "✅ Ready: data\\tl_2021_53_tract\n",
      "⬇️  Downloading & unzipping: https://www2.census.gov/geo/tiger/TIGER2021/PLACE/tl_2021_53_place.zip\n",
      "✅ Ready: data\\tl_2021_53_place\n",
      "✅ Wrote Seattle boundary: data\\seattle_boundary.geojson\n",
      "OD columns: ['w_geocode', 'h_geocode', 'S000', 'SA01', 'SA02', 'SA03', 'SE01', 'SE02', 'SE03', 'SI01']\n",
      "Xwalk columns: ['tabblk2020', 'st', 'stusps', 'stname', 'cty', 'ctyname', 'trct', 'trctname', 'bgrp', 'bgrpname']\n"
     ]
    }
   ],
   "source": [
    "# ===== Auto-download data (LODES8 2022 + Census TIGER) =====\n",
    "# pip install pandas geopandas shapely fiona pyproj requests\n",
    "\n",
    "import os, zipfile, io, requests, pandas as pd, geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path(\"data\"); DATA.mkdir(exist_ok=True)\n",
    "\n",
    "def fetch(url: str, out_path: Path):\n",
    "    if out_path.exists():\n",
    "        print(f\"✅ Exists: {out_path.name}\")\n",
    "        return out_path\n",
    "    print(f\"⬇️  Downloading: {url}\")\n",
    "    r = requests.get(url, stream=True, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    out_path.write_bytes(r.content)\n",
    "    print(f\"✅ Saved: {out_path.name}\")\n",
    "    return out_path\n",
    "\n",
    "def fetch_unzip(url: str, out_dir: Path):\n",
    "    out_dir.mkdir(exist_ok=True, parents=True)\n",
    "    marker = out_dir / \".unzipped\"\n",
    "    if marker.exists():\n",
    "        print(f\"✅ Unzipped: {out_dir.name}\")\n",
    "        return out_dir\n",
    "    print(f\"⬇️  Downloading & unzipping: {url}\")\n",
    "    r = requests.get(url, stream=True, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
    "        z.extractall(out_dir)\n",
    "    marker.touch()\n",
    "    print(f\"✅ Ready: {out_dir}\")\n",
    "    return out_dir\n",
    "\n",
    "# --- LODES8 (WA) OD + crosswalk (2022) ---\n",
    "od_url  = \"https://lehd.ces.census.gov/data/lodes/LODES8/wa/od/wa_od_main_JT00_2022.csv.gz\"\n",
    "xw_url  = \"https://lehd.ces.census.gov/data/lodes/LODES8/wa/wa_xwalk.csv.gz\"\n",
    "od_fp   = fetch(od_url, DATA / \"wa_od_main_JT00_2022.csv.gz\")\n",
    "xw_fp   = fetch(xw_url, DATA / \"wa_xwalk.csv.gz\")\n",
    "\n",
    "# --- TIGER/Line 2021 tracts (Washington; FIPS 53) ---\n",
    "tracts_dir = fetch_unzip(\n",
    "    \"https://www2.census.gov/geo/tiger/TIGER2021/TRACT/tl_2021_53_tract.zip\",\n",
    "    DATA / \"tl_2021_53_tract\"\n",
    ")\n",
    "\n",
    "# --- TIGER/Line 2021 places (to extract Seattle boundary programmatically) ---\n",
    "places_dir = fetch_unzip(\n",
    "    \"https://www2.census.gov/geo/tiger/TIGER2021/PLACE/tl_2021_53_place.zip\",\n",
    "    DATA / \"tl_2021_53_place\"\n",
    ")\n",
    "\n",
    "# Build Seattle city boundary from TIGER places (no need to hunt for a GeoJSON)\n",
    "places = gpd.read_file(places_dir / \"tl_2021_53_place.shp\").to_crs(4326)\n",
    "seattle = places.query(\"NAME == 'Seattle'\").copy()\n",
    "assert len(seattle) == 1, \"Seattle boundary not found or multiple matches.\"\n",
    "seattle_fp = DATA / \"seattle_boundary.geojson\"\n",
    "seattle.to_file(seattle_fp, driver=\"GeoJSON\")\n",
    "print(f\"✅ Wrote Seattle boundary: {seattle_fp}\")\n",
    "\n",
    "# Quick peek of LODES files (optional)\n",
    "od = pd.read_csv(od_fp, nrows=5); xw = pd.read_csv(xw_fp, nrows=5)\n",
    "print(\"OD columns:\", od.columns.tolist()[:10])\n",
    "print(\"Xwalk columns:\", xw.columns.tolist()[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "501b90c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OD sample cols: ['w_geocode', 'h_geocode', 'S000', 'SA01', 'SA02', 'SA03', 'SE01', 'SE02', 'SE03', 'SI01', 'SI02', 'SI03']\n"
     ]
    }
   ],
   "source": [
    "# ---- Step 1: Load LODES8 OD (keep geocodes as strings to preserve leading zeros)\n",
    "import pandas as pd, geopandas as gpd\n",
    "\n",
    "od = pd.read_csv(\n",
    "    \"data/wa_od_main_JT00_2022.csv.gz\",\n",
    "    dtype={\"h_geocode\":\"string\", \"w_geocode\":\"string\"}\n",
    ")\n",
    "# S000 = all jobs; keep as int for aggregation\n",
    "od[\"S000\"] = od[\"S000\"].astype(\"int64\")\n",
    "\n",
    "print(\"OD sample cols:\", od.columns.tolist()[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f99da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lette\\AppData\\Local\\Temp\\ipykernel_524152\\2638808382.py:2: DtypeWarning: Columns (31,35) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  xw = pd.read_csv(\"data/wa_xwalk.csv.gz\", dtype={\"tabblk2020\":\"string\",\"st\":\"string\",\"cty\":\"string\",\"trct\":\"string\"})\n"
     ]
    }
   ],
   "source": [
    "# ---- Step 2: Load crosswalk & build tract GEOIDs from st+cty+trct\n",
    "xw = pd.read_csv(\"data/wa_xwalk.csv.gz\", dtype={\"tabblk2020\":\"string\",\"st\":\"string\",\"cty\":\"string\",\"trct\":\"string\"})\n",
    "keep_cols = [\"tabblk2020\",\"st\",\"cty\",\"trct\",\"blklatdd\",\"blklondd\"]\n",
    "xw = xw[keep_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "848c7ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-pad & construct tract geoid (2 + 3 + 6 = 11 digits)\n",
    "xw[\"st\"]  = xw[\"st\"].str.zfill(2)\n",
    "xw[\"cty\"] = xw[\"cty\"].str.zfill(3)\n",
    "xw[\"trct\"] = xw[\"trct\"].str.zfill(6)\n",
    "xw[\"tract_geoid\"] = xw[\"st\"] + xw[\"cty\"] + xw[\"trct\"]\n",
    "\n",
    "xw_home = xw[[\"tabblk2020\",\"tract_geoid\"]].rename(columns={\"tabblk2020\":\"h_geocode\",\"tract_geoid\":\"tract_home\"})\n",
    "xw_work = xw[[\"tabblk2020\",\"tract_geoid\"]].rename(columns={\"tabblk2020\":\"w_geocode\",\"tract_geoid\":\"tract_work\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "129448e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing tract_home: 0.000% | Missing tract_work: 0.000%\n"
     ]
    }
   ],
   "source": [
    "# ---- Step 3: Attach tract IDs to OD (home & work)\n",
    "od = od.merge(xw_home, on=\"h_geocode\", how=\"left\")\n",
    "od = od.merge(xw_work, on=\"w_geocode\", how=\"left\", suffixes=(\"\",\"\"))\n",
    "\n",
    "# sanity check: any missing tract IDs?\n",
    "missing_home = od[\"tract_home\"].isna().mean()\n",
    "missing_work = od[\"tract_work\"].isna().mean()\n",
    "print(f\"Missing tract_home: {missing_home:.3%} | Missing tract_work: {missing_work:.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98bf43e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow pairs (tract→tract): 668211\n"
     ]
    }
   ],
   "source": [
    "# ---- Step 4: Aggregate to tract-to-tract flows\n",
    "flows = (od.groupby([\"tract_home\",\"tract_work\"], dropna=False, as_index=False)[\"S000\"].sum()\n",
    "           .rename(columns={\"S000\":\"jobs\"}))\n",
    "\n",
    "# drop OD pairs with missing tract ids just in case\n",
    "flows = flows.dropna(subset=[\"tract_home\",\"tract_work\"]).copy()\n",
    "\n",
    "print(\"Flow pairs (tract→tract):\", len(flows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4638dd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lette\\AppData\\Local\\Temp\\ipykernel_524152\\4064277167.py:4: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  tracts[\"lon\"] = tracts.geometry.centroid.x\n",
      "C:\\Users\\lette\\AppData\\Local\\Temp\\ipykernel_524152\\4064277167.py:5: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  tracts[\"lat\"] = tracts.geometry.centroid.y\n"
     ]
    }
   ],
   "source": [
    "# ---- Step 5: Load WA tracts (TIGER/Line 2021) and compute centroids\n",
    "tracts = gpd.read_file(\"data/tl_2021_53_tract/tl_2021_53_tract.shp\").to_crs(4326)\n",
    "tracts = tracts.rename(columns={\"GEOID\":\"tract\"}).loc[:, [\"tract\",\"geometry\"]].copy()\n",
    "tracts[\"lon\"] = tracts.geometry.centroid.x\n",
    "tracts[\"lat\"] = tracts.geometry.centroid.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f89bc107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 6: Attach origin/destination coords to flows\n",
    "flows = flows.merge(tracts[[\"tract\",\"lon\",\"lat\"]], left_on=\"tract_home\", right_on=\"tract\", how=\"left\")\n",
    "flows = flows.rename(columns={\"lon\":\"home_lon\",\"lat\":\"home_lat\"}).drop(columns=[\"tract\"])\n",
    "\n",
    "flows = flows.merge(tracts[[\"tract\",\"lon\",\"lat\"]], left_on=\"tract_work\", right_on=\"tract\", how=\"left\")\n",
    "flows = flows.rename(columns={\"lon\":\"work_lon\",\"lat\":\"work_lat\"}).drop(columns=[\"tract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12dc991d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lette\\AppData\\Local\\Temp\\ipykernel_524152\\1498783637.py:4: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  flows_seattle = flows_gdf[flows_gdf.within(seattle.unary_union)].copy()\n"
     ]
    }
   ],
   "source": [
    "# ---- Step 7: Filter to flows with WORK in Seattle (build Seattle from TIGER Places you saved earlier)\n",
    "seattle = gpd.read_file(\"data/seattle_boundary.geojson\").to_crs(4326)\n",
    "flows_gdf = gpd.GeoDataFrame(flows, geometry=gpd.points_from_xy(flows[\"work_lon\"], flows[\"work_lat\"]), crs=4326)\n",
    "flows_seattle = flows_gdf[flows_gdf.within(seattle.unary_union)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0d400fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported for Kepler.gl: data/seattle_flows_kepler_2022.csv\n"
     ]
    }
   ],
   "source": [
    "# ---- Step 8: Export for Kepler.gl Arc Layer\n",
    "out_csv = \"data/seattle_flows_kepler_2022.csv\"\n",
    "flows_seattle[[\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\",\"jobs\"]].to_csv(out_csv, index=False)\n",
    "print(f\"✅ Exported for Kepler.gl: {out_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984be701",
   "metadata": {},
   "source": [
    "# ReRunning the code to export OD data with block, blockgroup, tract level flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5967e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Seattle Commuting Flows from LODES8 (WA, 2022)\n",
    "# - Block, Block Group, and Tract level\n",
    "# - Filtered to jobs with WORK in Seattle city boundary\n",
    "# ==========================================================\n",
    "\n",
    "import pandas as pd, geopandas as gpd, requests, zipfile, io\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path(\"data\"); DATA.mkdir(exist_ok=True)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1) Fetch OD + Crosswalk + TIGER (tracts, block groups, places)\n",
    "# ----------------------------------------------------------\n",
    "def fetch(url, out_path):\n",
    "    if not out_path.exists():\n",
    "        print(\"⬇️\", url)\n",
    "        r = requests.get(url, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        out_path.write_bytes(r.content)\n",
    "    return out_path\n",
    "\n",
    "def fetch_unzip(url, out_dir):\n",
    "    shp_path = list(out_dir.glob(\"*.shp\"))\n",
    "    if shp_path: return shp_path[0]\n",
    "    print(\"⬇️\", url)\n",
    "    r = requests.get(url, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content)) as z: z.extractall(out_dir)\n",
    "    return list(out_dir.glob(\"*.shp\"))[0]\n",
    "\n",
    "od_fp  = fetch(\"https://lehd.ces.census.gov/data/lodes/LODES8/wa/od/wa_od_main_JT00_2022.csv.gz\",\n",
    "               DATA/\"wa_od_main_JT00_2022.csv.gz\")\n",
    "xw_fp  = fetch(\"https://lehd.ces.census.gov/data/lodes/LODES8/wa/wa_xwalk.csv.gz\",\n",
    "               DATA/\"wa_xwalk.csv.gz\")\n",
    "\n",
    "tracts_shp = fetch_unzip(\"https://www2.census.gov/geo/tiger/TIGER2021/TRACT/tl_2021_53_tract.zip\",\n",
    "                         DATA/\"tl_2021_53_tract\")\n",
    "bg_shp     = fetch_unzip(\"https://www2.census.gov/geo/tiger/TIGER2021/BG/tl_2021_53_bg.zip\",\n",
    "                         DATA/\"tl_2021_53_bg\")\n",
    "places_shp = fetch_unzip(\"https://www2.census.gov/geo/tiger/TIGER2021/PLACE/tl_2021_53_place.zip\",\n",
    "                         DATA/\"tl_2021_53_place\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51dd436a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lette\\AppData\\Local\\Temp\\ipykernel_524152\\2748129456.py:7: DtypeWarning: Columns (31,35) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  xw = pd.read_csv(xw_fp, dtype={\"tabblk2020\":\"string\",\"st\":\"string\",\"cty\":\"string\",\"trct\":\"string\"})\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 2) Load OD + Crosswalk\n",
    "# ----------------------------------------------------------\n",
    "od = pd.read_csv(od_fp, dtype={\"h_geocode\":\"string\",\"w_geocode\":\"string\"})\n",
    "od[\"S000\"] = od[\"S000\"].astype(\"int64\")\n",
    "\n",
    "xw = pd.read_csv(xw_fp, dtype={\"tabblk2020\":\"string\",\"st\":\"string\",\"cty\":\"string\",\"trct\":\"string\"})\n",
    "xw[\"st\"]   = xw[\"st\"].str.zfill(2)\n",
    "xw[\"cty\"]  = xw[\"cty\"].str.zfill(3)\n",
    "xw[\"trct\"] = xw[\"trct\"].str.zfill(6)\n",
    "xw[\"tract_geoid\"] = xw[\"st\"] + xw[\"cty\"] + xw[\"trct\"]\n",
    "xw[\"block_code4\"] = xw[\"tabblk2020\"].str[-4:]\n",
    "xw[\"bgrp\"] = xw[\"block_code4\"].str[0]\n",
    "xw[\"bg_geoid\"] = xw[\"tract_geoid\"] + xw[\"bgrp\"]\n",
    "\n",
    "# lookups\n",
    "blk_coords = xw.rename(columns={\"tabblk2020\":\"geocode\",\"blklondd\":\"lon\",\"blklatdd\":\"lat\"})[[\"geocode\",\"lon\",\"lat\"]]\n",
    "blk_to_bg  = xw[[\"tabblk2020\",\"bg_geoid\"]].rename(columns={\"tabblk2020\":\"geocode\"})\n",
    "blk_to_tr  = xw[[\"tabblk2020\",\"tract_geoid\"]].rename(columns={\"tabblk2020\":\"geocode\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b4364f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seattle boundary loaded.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 3) Seattle boundary\n",
    "# ----------------------------------------------------------\n",
    "places = gpd.read_file(places_shp).to_crs(4326)\n",
    "seattle = places.query(\"NAME == 'Seattle'\").copy()\n",
    "print(\"Seattle boundary loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37eaf684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lette\\AppData\\Local\\Temp\\ipykernel_524152\\194652151.py:16: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  flows_block_sea = flows_block_gdf[flows_block_gdf.within(seattle.unary_union)].dropna(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block-level flows: 544781\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 4) Block → Block flows\n",
    "# ----------------------------------------------------------\n",
    "flows_block = od[[\"h_geocode\",\"w_geocode\",\"S000\"]].rename(columns={\"S000\":\"jobs\"})\n",
    "\n",
    "# attach coords\n",
    "flows_block = flows_block.merge(blk_coords, left_on=\"h_geocode\", right_on=\"geocode\", how=\"left\") \\\n",
    "                         .rename(columns={\"lon\":\"home_lon\",\"lat\":\"home_lat\"}).drop(columns=[\"geocode\"])\n",
    "flows_block = flows_block.merge(blk_coords, left_on=\"w_geocode\", right_on=\"geocode\", how=\"left\") \\\n",
    "                         .rename(columns={\"lon\":\"work_lon\",\"lat\":\"work_lat\"}).drop(columns=[\"geocode\"])\n",
    "\n",
    "# filter Seattle destinations\n",
    "flows_block_gdf = gpd.GeoDataFrame(flows_block,\n",
    "                                   geometry=gpd.points_from_xy(flows_block[\"work_lon\"], flows_block[\"work_lat\"]),\n",
    "                                   crs=4326)\n",
    "flows_block_sea = flows_block_gdf[flows_block_gdf.within(seattle.unary_union)].dropna(\n",
    "    subset=[\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\"]\n",
    ").copy()\n",
    "flows_block_sea[\"geo_level\"] = \"block\"\n",
    "print(\"Block-level flows:\", len(flows_block_sea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f29374d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lette\\AppData\\Local\\Temp\\ipykernel_524152\\3795449847.py:14: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  bg[\"lon\"] = bg.geometry.centroid.x\n",
      "C:\\Users\\lette\\AppData\\Local\\Temp\\ipykernel_524152\\3795449847.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  bg[\"lat\"] = bg.geometry.centroid.y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block-group flows: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lette\\AppData\\Local\\Temp\\ipykernel_524152\\3795449847.py:25: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  flows_bg_sea = flows_bg_gdf[flows_bg_gdf.within(seattle.unary_union)].dropna(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 5) Block Group → Block Group flows\n",
    "# ----------------------------------------------------------\n",
    "od_bg = od.merge(blk_to_bg.rename(columns={\"geocode\":\"h_geocode\",\"bg_geoid\":\"bg_home\"}),\n",
    "                 on=\"h_geocode\", how=\"left\") \\\n",
    "          .merge(blk_to_bg.rename(columns={\"geocode\":\"w_geocode\",\"bg_geoid\":\"bg_work\"}),\n",
    "                 on=\"w_geocode\", how=\"left\")\n",
    "\n",
    "flows_bg = (od_bg.groupby([\"bg_home\",\"bg_work\"], as_index=False)[\"S000\"].sum()\n",
    "                 .rename(columns={\"S000\":\"jobs\"}))\n",
    "\n",
    "# load BG shapes for centroids\n",
    "bg = gpd.read_file(bg_shp).to_crs(4326).rename(columns={\"GEOID\":\"bg_geoid\"})\n",
    "bg[\"lon\"] = bg.geometry.centroid.x\n",
    "bg[\"lat\"] = bg.geometry.centroid.y\n",
    "\n",
    "flows_bg = flows_bg.merge(bg[[\"bg_geoid\",\"lon\",\"lat\"]], left_on=\"bg_home\", right_on=\"bg_geoid\", how=\"left\") \\\n",
    "                   .rename(columns={\"lon\":\"home_lon\",\"lat\":\"home_lat\"}).drop(columns=[\"bg_geoid\"])\n",
    "flows_bg = flows_bg.merge(bg[[\"bg_geoid\",\"lon\",\"lat\"]], left_on=\"bg_work\", right_on=\"bg_geoid\", how=\"left\") \\\n",
    "                   .rename(columns={\"lon\":\"work_lon\",\"lat\":\"work_lat\"}).drop(columns=[\"bg_geoid\"])\n",
    "\n",
    "flows_bg_gdf = gpd.GeoDataFrame(flows_bg,\n",
    "                                geometry=gpd.points_from_xy(flows_bg[\"work_lon\"], flows_bg[\"work_lat\"]),\n",
    "                                crs=4326)\n",
    "flows_bg_sea = flows_bg_gdf[flows_bg_gdf.within(seattle.unary_union)].dropna(\n",
    "    subset=[\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\"]\n",
    ").copy()\n",
    "flows_bg_sea[\"geo_level\"] = \"blockgroup\"\n",
    "print(\"Block-group flows:\", len(flows_bg_sea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ac30c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lette\\AppData\\Local\\Temp\\ipykernel_524152\\3824834592.py:14: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  tracts[\"lon\"] = tracts.geometry.centroid.x\n",
      "C:\\Users\\lette\\AppData\\Local\\Temp\\ipykernel_524152\\3824834592.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  tracts[\"lat\"] = tracts.geometry.centroid.y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tract-level flows: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lette\\AppData\\Local\\Temp\\ipykernel_524152\\3824834592.py:25: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  flows_tr_sea = flows_tr_gdf[flows_tr_gdf.within(seattle.unary_union)].dropna(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 6) Tract → Tract flows\n",
    "# ----------------------------------------------------------\n",
    "od_tr = od.merge(blk_to_tr.rename(columns={\"geocode\":\"h_geocode\",\"tract_geoid\":\"tract_home\"}),\n",
    "                 on=\"h_geocode\", how=\"left\") \\\n",
    "          .merge(blk_to_tr.rename(columns={\"geocode\":\"w_geocode\",\"tract_geoid\":\"tract_work\"}),\n",
    "                 on=\"w_geocode\", how=\"left\")\n",
    "od_tr = od_tr.dropna(subset=[\"tract_home\",\"tract_work\"])\n",
    "\n",
    "flows_tr = (od_tr.groupby([\"tract_home\",\"tract_work\"], as_index=False)[\"S000\"].sum()\n",
    "                 .rename(columns={\"S000\":\"jobs\"}))\n",
    "\n",
    "tracts = gpd.read_file(tracts_shp).to_crs(4326).rename(columns={\"GEOID\":\"tract\"})\n",
    "tracts[\"lon\"] = tracts.geometry.centroid.x\n",
    "tracts[\"lat\"] = tracts.geometry.centroid.y\n",
    "\n",
    "flows_tr = flows_tr.merge(tracts[[\"tract\",\"lon\",\"lat\"]], left_on=\"tract_home\", right_on=\"tract\", how=\"left\") \\\n",
    "                   .rename(columns={\"lon\":\"home_lon\",\"lat\":\"home_lat\"}).drop(columns=[\"tract\"])\n",
    "flows_tr = flows_tr.merge(tracts[[\"tract\",\"lon\",\"lat\"]], left_on=\"tract_work\", right_on=\"tract\", how=\"left\") \\\n",
    "                   .rename(columns={\"lon\":\"work_lon\",\"lat\":\"work_lat\"}).drop(columns=[\"tract\"])\n",
    "\n",
    "flows_tr_gdf = gpd.GeoDataFrame(flows_tr,\n",
    "                                geometry=gpd.points_from_xy(flows_tr[\"work_lon\"], flows_tr[\"work_lat\"]),\n",
    "                                crs=4326)\n",
    "flows_tr_sea = flows_tr_gdf[flows_tr_gdf.within(seattle.unary_union)].dropna(\n",
    "    subset=[\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\"]\n",
    ").copy()\n",
    "flows_tr_sea[\"geo_level\"] = \"tract\"\n",
    "print(\"Tract-level flows:\", len(flows_tr_sea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "971bd706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved all CSVs in data/ folder\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 7) Save outputs\n",
    "# ----------------------------------------------------------\n",
    "cols = [\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\",\"jobs\",\"geo_level\"]\n",
    "\n",
    "flows_block_sea[cols].to_csv(DATA/\"seattle_flows_block_2022.csv\", index=False)\n",
    "flows_bg_sea[cols].to_csv(DATA/\"seattle_flows_blockgroup_2022.csv\", index=False)\n",
    "flows_tr_sea[cols].to_csv(DATA/\"seattle_flows_tract_2022.csv\", index=False)\n",
    "\n",
    "flows_all = pd.concat([flows_block_sea[cols], flows_bg_sea[cols], flows_tr_sea[cols]], ignore_index=True)\n",
    "flows_all.to_csv(DATA/\"seattle_flows_multilevel_2022.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved all CSVs in data/ folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "023fdd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['w_geocode', 'h_geocode', 'S000', 'SA01', 'SA02', 'SA03', 'SE01',\n",
      "       'SE02', 'SE03', 'SI01', 'SI02', 'SI03', 'createdate'],\n",
      "      dtype='object')\n",
      "         h_geocode        w_geocode  S000\n",
      "0  530019501001039  530019501001000     2\n",
      "1  530019502001089  530019501001000     1\n",
      "2  530750003002015  530019501001000     1\n",
      "3  530750003004008  530019501001000     1\n",
      "4  530019501002042  530019501001011     1\n",
      "count    5.000000\n",
      "mean     1.200000\n",
      "std      0.447214\n",
      "min      1.000000\n",
      "25%      1.000000\n",
      "50%      1.000000\n",
      "75%      1.000000\n",
      "max      2.000000\n",
      "Name: S000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "sample = pd.read_csv(\"data/wa_od_main_JT00_2022.csv.gz\", nrows=5)\n",
    "print(sample.columns)\n",
    "print(sample[[\"h_geocode\",\"w_geocode\",\"S000\"]])\n",
    "print(sample[\"S000\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86df0498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tract flows created: 668211\n",
      "           tract_home          tract_work  jobs\n",
      "0  535300153001950100  535300153001950100   401\n",
      "1  535300153001950100  535300153001950200    62\n",
      "2  535300153001950100  535300153001950301     2\n",
      "3  535300153001950100  535300153001950302     1\n",
      "4  535300153001950100  535300153001950400     6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lette\\AppData\\Local\\Temp\\ipykernel_524152\\1902839978.py:43: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  tracts[\"lon\"] = tracts.geometry.centroid.x\n",
      "C:\\Users\\lette\\AppData\\Local\\Temp\\ipykernel_524152\\1902839978.py:44: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  tracts[\"lat\"] = tracts.geometry.centroid.y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seattle tract flows: 0\n",
      "Empty GeoDataFrame\n",
      "Columns: [tract_home, tract_work, jobs, home_lon, home_lat, work_lon, work_lat, geometry, geo_level]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Rebuild tract → tract flows (Seattle work destinations)\n",
    "# ==========================================================\n",
    "\n",
    "import pandas as pd, geopandas as gpd\n",
    "\n",
    "# Ensure OD geocodes are strings\n",
    "od[\"h_geocode\"] = od[\"h_geocode\"].astype(str)\n",
    "od[\"w_geocode\"] = od[\"w_geocode\"].astype(str)\n",
    "\n",
    "# Build block → tract lookup from crosswalk\n",
    "xw[\"st\"]   = xw[\"st\"].astype(str).str.zfill(2)\n",
    "xw[\"cty\"]  = xw[\"cty\"].astype(str).str.zfill(3)\n",
    "xw[\"trct\"] = xw[\"trct\"].astype(str).str.zfill(6)\n",
    "xw[\"tract_geoid\"] = xw[\"st\"] + xw[\"cty\"] + xw[\"trct\"]\n",
    "\n",
    "blk_to_tr = xw[[\"tabblk2020\",\"tract_geoid\"]].rename(columns={\"tabblk2020\":\"geocode\"})\n",
    "blk_to_tr[\"geocode\"] = blk_to_tr[\"geocode\"].astype(str)\n",
    "\n",
    "# Attach tract IDs to OD\n",
    "od_tr = (\n",
    "    od.merge(blk_to_tr.rename(columns={\"geocode\":\"h_geocode\",\"tract_geoid\":\"tract_home\"}),\n",
    "             on=\"h_geocode\", how=\"left\")\n",
    "      .merge(blk_to_tr.rename(columns={\"geocode\":\"w_geocode\",\"tract_geoid\":\"tract_work\"}),\n",
    "             on=\"w_geocode\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Drop missing tract IDs\n",
    "od_tr = od_tr.dropna(subset=[\"tract_home\",\"tract_work\"])\n",
    "\n",
    "# Aggregate to tract → tract jobs\n",
    "flows_tr = (\n",
    "    od_tr.groupby([\"tract_home\",\"tract_work\"], as_index=False)[\"S000\"].sum()\n",
    "          .rename(columns={\"S000\":\"jobs\"})\n",
    ")\n",
    "\n",
    "print(\"Tract flows created:\", len(flows_tr))\n",
    "print(flows_tr.head())\n",
    "\n",
    "# --- Attach tract centroids\n",
    "tracts = gpd.read_file(\"data/tl_2021_53_tract/tl_2021_53_tract.shp\").to_crs(4326)\n",
    "tracts[\"GEOID\"] = tracts[\"GEOID\"].astype(str).str.zfill(11)\n",
    "tracts[\"lon\"] = tracts.geometry.centroid.x\n",
    "tracts[\"lat\"] = tracts.geometry.centroid.y\n",
    "\n",
    "flows_tr = flows_tr.merge(tracts[[\"GEOID\",\"lon\",\"lat\"]],\n",
    "                          left_on=\"tract_home\", right_on=\"GEOID\", how=\"left\") \\\n",
    "                   .rename(columns={\"lon\":\"home_lon\",\"lat\":\"home_lat\"}).drop(columns=[\"GEOID\"])\n",
    "flows_tr = flows_tr.merge(tracts[[\"GEOID\",\"lon\",\"lat\"]],\n",
    "                          left_on=\"tract_work\", right_on=\"GEOID\", how=\"left\") \\\n",
    "                   .rename(columns={\"lon\":\"work_lon\",\"lat\":\"work_lat\"}).drop(columns=[\"GEOID\"])\n",
    "\n",
    "# --- Filter to WORK in Seattle\n",
    "flows_tr_gdf = gpd.GeoDataFrame(\n",
    "    flows_tr,\n",
    "    geometry=gpd.points_from_xy(flows_tr[\"work_lon\"], flows_tr[\"work_lat\"]),\n",
    "    crs=4326\n",
    ")\n",
    "flows_tr_sea = flows_tr_gdf[flows_tr_gdf.within(seattle.union_all())].dropna(\n",
    "    subset=[\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\"]\n",
    ").copy()\n",
    "flows_tr_sea[\"geo_level\"] = \"tract\"\n",
    "\n",
    "print(\"Seattle tract flows:\", len(flows_tr_sea))\n",
    "print(flows_tr_sea.sort_values(\"jobs\", ascending=False).head())\n",
    "\n",
    "# Save CSV\n",
    "flows_tr_sea[[\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\",\"jobs\",\"geo_level\"]] \\\n",
    "    .to_csv(\"data/seattle_flows_tract_2022.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "556ac633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       0\n",
       "unique      0\n",
       "top       NaN\n",
       "freq      NaN\n",
       "Name: jobs, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data/seattle_flows_tract_2022.csv\")[\"jobs\"].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6932b10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seattle polygons: 1\n",
      "Tracts intersecting Seattle: 199\n",
      "   STATEFP COUNTYFP TRACTCE        GEOID    NAME             NAMELSAD  MTFCC  \\\n",
      "67      53      033  001701  53033001701   17.01   Census Tract 17.01  G5020   \n",
      "68      53      033  010401  53033010401  104.01  Census Tract 104.01  G5020   \n",
      "79      53      033  010701  53033010701  107.01  Census Tract 107.01  G5020   \n",
      "80      53      033  011001  53033011001  110.01  Census Tract 110.01  G5020   \n",
      "96      53      033  004301  53033004301   43.01   Census Tract 43.01  G5020   \n",
      "\n",
      "   FUNCSTAT    ALAND  AWATER     INTPTLAT      INTPTLON  \\\n",
      "67        S   892768       0  +47.6963619  -122.3544843   \n",
      "68        S  1629874       0  +47.5532095  -122.2955875   \n",
      "79        S  1457384    3664  +47.5451419  -122.3675265   \n",
      "80        S  1080391       0  +47.5384101  -122.2861704   \n",
      "96        S   979236       0  +47.6694019  -122.3002215   \n",
      "\n",
      "                                             geometry         lon        lat  \n",
      "67  POLYGON ((-122.36335 47.69425, -122.36201 47.6... -122.354483  47.696364  \n",
      "68  POLYGON ((-122.30439 47.55964, -122.30367 47.5... -122.295586  47.553212  \n",
      "79  POLYGON ((-122.3723 47.5416, -122.37227 47.542... -122.366048  47.542910  \n",
      "80  POLYGON ((-122.29279 47.54432, -122.29264 47.5... -122.286169  47.538412  \n",
      "96  POLYGON ((-122.30652 47.67578, -122.30547 47.6... -122.300220  47.669404  \n"
     ]
    }
   ],
   "source": [
    "print(\"Seattle polygons:\", seattle.shape[0])\n",
    "\n",
    "# Intersect tracts with Seattle boundary\n",
    "tracts_in_sea = tracts[tracts.intersects(seattle.union_all())]\n",
    "print(\"Tracts intersecting Seattle:\", len(tracts_in_sea))\n",
    "\n",
    "print(tracts_in_sea.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca7c3c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exists: wa_od_main_JT00_2022.csv.gz\n",
      "✅ Exists: wa_xwalk.csv.gz\n",
      "✅ Exists: tl_2021_53_tract\n",
      "✅ Exists: tl_2021_53_bg\n",
      "✅ Exists: tl_2021_53_place\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lette\\AppData\\Local\\Temp\\ipykernel_618512\\2525981454.py:70: DtypeWarning: Columns (31,35) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  xw = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seattle boundary polygons: 1\n",
      "Block-level flows: 544781\n",
      "Block-group flows: 0\n",
      "Tract-level flows: 0\n",
      "✅ Saved:\n",
      " - data/seattle_flows_block_2022.csv\n",
      " - data/seattle_flows_blockgroup_2022.csv\n",
      " - data/seattle_flows_tract_2022.csv\n",
      " - data/seattle_flows_multilevel_2022.csv  (all levels in one file)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Seattle Commuting Flows (LODES8, WA 2022)\n",
    "# - Block, Block Group, and Tract levels\n",
    "# - Destinations filtered to polygons that INTERSECT Seattle\n",
    "# - Centroids computed in projected CRS to avoid warnings\n",
    "# - Exports: block / blockgroup / tract CSVs + combined CSV\n",
    "# ==========================================================\n",
    "\n",
    "import os, io, zipfile, requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# -----------------------------\n",
    "# Config & helpers\n",
    "# -----------------------------\n",
    "DATA = Path(\"data\"); DATA.mkdir(exist_ok=True)\n",
    "\n",
    "def fetch(url: str, out_path: Path):\n",
    "    if out_path.exists():\n",
    "        print(f\"✅ Exists: {out_path.name}\")\n",
    "        return out_path\n",
    "    print(f\"⬇️  Downloading: {url}\")\n",
    "    r = requests.get(url, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    out_path.write_bytes(r.content)\n",
    "    print(f\"✅ Saved: {out_path.name}\")\n",
    "    return out_path\n",
    "\n",
    "def fetch_unzip(url: str, out_dir: Path) -> Path:\n",
    "    out_dir.mkdir(exist_ok=True, parents=True)\n",
    "    shp = list(out_dir.glob(\"*.shp\"))\n",
    "    if shp:\n",
    "        print(f\"✅ Exists: {out_dir.name}\")\n",
    "        return shp[0]\n",
    "    print(f\"⬇️  Downloading & unzipping: {url}\")\n",
    "    r = requests.get(url, timeout=180)\n",
    "    r.raise_for_status()\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
    "        z.extractall(out_dir)\n",
    "    shp = list(out_dir.glob(\"*.shp\"))\n",
    "    print(f\"✅ Unzipped: {out_dir} ({shp[0].name})\")\n",
    "    return shp[0]\n",
    "\n",
    "def geounion(gdf: gpd.GeoDataFrame):\n",
    "    # use union_all() if available; fallback to unary_union for older versions\n",
    "    return gdf.union_all() if hasattr(gdf, \"union_all\") else gdf.unary_union\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Download inputs\n",
    "# -----------------------------\n",
    "od_fp  = fetch(\"https://lehd.ces.census.gov/data/lodes/LODES8/wa/od/wa_od_main_JT00_2022.csv.gz\",\n",
    "               DATA / \"wa_od_main_JT00_2022.csv.gz\")\n",
    "xw_fp  = fetch(\"https://lehd.ces.census.gov/data/lodes/LODES8/wa/wa_xwalk.csv.gz\",\n",
    "               DATA / \"wa_xwalk.csv.gz\")\n",
    "\n",
    "tracts_shp  = fetch_unzip(\"https://www2.census.gov/geo/tiger/TIGER2021/TRACT/tl_2021_53_tract.zip\",\n",
    "                          DATA / \"tl_2021_53_tract\")\n",
    "bg_shp      = fetch_unzip(\"https://www2.census.gov/geo/tiger/TIGER2021/BG/tl_2021_53_bg.zip\",\n",
    "                          DATA / \"tl_2021_53_bg\")\n",
    "places_shp  = fetch_unzip(\"https://www2.census.gov/geo/tiger/TIGER2021/PLACE/tl_2021_53_place.zip\",\n",
    "                          DATA / \"tl_2021_53_place\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Load OD + crosswalk\n",
    "# -----------------------------\n",
    "od = pd.read_csv(od_fp, dtype={\"h_geocode\":\"string\",\"w_geocode\":\"string\"})\n",
    "od[\"S000\"] = od[\"S000\"].astype(\"int64\")\n",
    "\n",
    "xw = pd.read_csv(\n",
    "    xw_fp,\n",
    "    dtype={\"tabblk2020\":\"string\",\"st\":\"string\",\"cty\":\"string\",\"trct\":\"string\"}\n",
    ")\n",
    "\n",
    "# build tract + block-group GEOIDs from crosswalk (LODES8 schema)\n",
    "xw[\"st\"]   = xw[\"st\"].str.zfill(2)\n",
    "xw[\"cty\"]  = xw[\"cty\"].str.zfill(3)\n",
    "xw[\"trct\"] = xw[\"trct\"].str.zfill(6)\n",
    "xw[\"tract_geoid\"] = xw[\"st\"] + xw[\"cty\"] + xw[\"trct\"]\n",
    "\n",
    "xw[\"block_code4\"] = xw[\"tabblk2020\"].str[-4:]  # last 4 chars\n",
    "xw[\"bgrp\"] = xw[\"block_code4\"].str[0]         # first char of block => block group\n",
    "xw[\"bg_geoid\"] = xw[\"tract_geoid\"] + xw[\"bgrp\"]\n",
    "\n",
    "# handy lookups\n",
    "blk_coords = xw.rename(columns={\"tabblk2020\":\"geocode\",\n",
    "                                \"blklondd\":\"lon\",\n",
    "                                \"blklatdd\":\"lat\"})[[\"geocode\",\"lon\",\"lat\"]]\n",
    "blk_to_bg  = xw[[\"tabblk2020\",\"bg_geoid\"]].rename(columns={\"tabblk2020\":\"geocode\"})\n",
    "blk_to_tr  = xw[[\"tabblk2020\",\"tract_geoid\"]].rename(columns={\"tabblk2020\":\"geocode\"})\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Load TIGER shapes (WGS84) + Seattle polygon\n",
    "# -----------------------------\n",
    "tracts = gpd.read_file(tracts_shp).to_crs(4326)\n",
    "tracts[\"GEOID\"] = tracts[\"GEOID\"].astype(str).str.zfill(11)\n",
    "\n",
    "bg = gpd.read_file(bg_shp).to_crs(4326).rename(columns={\"GEOID\":\"bg_geoid\"})\n",
    "bg[\"bg_geoid\"] = bg[\"bg_geoid\"].astype(str).str.zfill(12)\n",
    "\n",
    "places = gpd.read_file(places_shp).to_crs(4326)\n",
    "seattle = places.query(\"NAME == 'Seattle'\").copy()\n",
    "assert len(seattle) == 1, \"Seattle boundary not found or multiple matches.\"\n",
    "SEA = geounion(seattle)  # unified polygon\n",
    "\n",
    "print(\"Seattle boundary polygons:\", len(seattle))\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Helper: accurate centroids (project, centroid, back)\n",
    "# -----------------------------\n",
    "def centroid_lonlat(gdf: gpd.GeoDataFrame, proj_epsg=3857):\n",
    "    # project to metric CRS for centroid calc\n",
    "    g_proj = gdf.to_crs(epsg=proj_epsg)\n",
    "    cents = g_proj.geometry.centroid\n",
    "    # back to lon/lat\n",
    "    cents_ll = gpd.GeoSeries(cents, crs=g_proj.crs).to_crs(4326)\n",
    "    return pd.DataFrame({\"lon\": cents_ll.x.values, \"lat\": cents_ll.y.values})\n",
    "\n",
    "# Precompute centroids for BG and tracts\n",
    "bg_cent = centroid_lonlat(bg)\n",
    "bg = pd.concat([bg.reset_index(drop=True), bg_cent], axis=1)\n",
    "\n",
    "tr_cent = centroid_lonlat(tracts)\n",
    "tracts = pd.concat([tracts.reset_index(drop=True), tr_cent], axis=1)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Block → Block flows (direct coords from crosswalk)\n",
    "# -----------------------------\n",
    "flows_block = od[[\"h_geocode\",\"w_geocode\",\"S000\"]].rename(columns={\"S000\":\"jobs\"})\n",
    "\n",
    "flows_block = flows_block.merge(blk_coords, left_on=\"h_geocode\", right_on=\"geocode\", how=\"left\") \\\n",
    "                         .rename(columns={\"lon\":\"home_lon\",\"lat\":\"home_lat\"}).drop(columns=[\"geocode\"])\n",
    "flows_block = flows_block.merge(blk_coords, left_on=\"w_geocode\", right_on=\"geocode\", how=\"left\") \\\n",
    "                         .rename(columns={\"lon\":\"work_lon\",\"lat\":\"work_lat\"}).drop(columns=[\"geocode\"])\n",
    "\n",
    "# polygon-based destination filter (intersects Seattle)\n",
    "flows_block_gdf = gpd.GeoDataFrame(\n",
    "    flows_block,\n",
    "    geometry=gpd.points_from_xy(flows_block[\"work_lon\"], flows_block[\"work_lat\"]),\n",
    "    crs=4326\n",
    ")\n",
    "flows_block_sea = flows_block_gdf[flows_block_gdf.intersects(SEA)].dropna(\n",
    "    subset=[\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\"]\n",
    ").copy()\n",
    "flows_block_sea[\"geo_level\"] = \"block\"\n",
    "print(\"Block-level flows:\", len(flows_block_sea))\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Block Group → Block Group flows (ID-based destination filter)\n",
    "# -----------------------------\n",
    "# Map blocks to BGs\n",
    "od_bg = (od.merge(blk_to_bg.rename(columns={\"geocode\":\"h_geocode\",\"bg_geoid\":\"bg_home\"}),\n",
    "                  on=\"h_geocode\", how=\"left\")\n",
    "           .merge(blk_to_bg.rename(columns={\"geocode\":\"w_geocode\",\"bg_geoid\":\"bg_work\"}),\n",
    "                  on=\"w_geocode\", how=\"left\"))\n",
    "od_bg = od_bg.dropna(subset=[\"bg_home\",\"bg_work\"])\n",
    "od_bg[\"bg_home\"] = od_bg[\"bg_home\"].astype(str).str.zfill(12)\n",
    "od_bg[\"bg_work\"] = od_bg[\"bg_work\"].astype(str).str.zfill(12)\n",
    "\n",
    "# Which BG polygons touch Seattle?\n",
    "bg_in_sea = set(bg[bg.intersects(SEA)][\"bg_geoid\"])\n",
    "od_bg = od_bg[od_bg[\"bg_work\"].isin(bg_in_sea)]\n",
    "\n",
    "# Aggregate BG→BG\n",
    "flows_bg = (od_bg.groupby([\"bg_home\",\"bg_work\"], as_index=False)[\"S000\"]\n",
    "                 .sum().rename(columns={\"S000\":\"jobs\"}))\n",
    "\n",
    "# Attach centroids\n",
    "flows_bg = flows_bg.merge(bg[[\"bg_geoid\",\"lon\",\"lat\"]],\n",
    "                          left_on=\"bg_home\", right_on=\"bg_geoid\", how=\"left\") \\\n",
    "                   .rename(columns={\"lon\":\"home_lon\",\"lat\":\"home_lat\"}).drop(columns=[\"bg_geoid\"])\n",
    "flows_bg = flows_bg.merge(bg[[\"bg_geoid\",\"lon\",\"lat\"]],\n",
    "                          left_on=\"bg_work\", right_on=\"bg_geoid\", how=\"left\") \\\n",
    "                   .rename(columns={\"lon\":\"work_lon\",\"lat\":\"work_lat\"}).drop(columns=[\"bg_geoid\"])\n",
    "\n",
    "flows_bg = flows_bg.dropna(subset=[\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\"]).copy()\n",
    "flows_bg[\"geo_level\"] = \"blockgroup\"\n",
    "print(\"Block-group flows:\", len(flows_bg))\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Tract → Tract flows (ID-based destination filter)\n",
    "# -----------------------------\n",
    "# Map blocks to tracts\n",
    "od_tr = (od.merge(blk_to_tr.rename(columns={\"geocode\":\"h_geocode\",\"tract_geoid\":\"tract_home\"}),\n",
    "                  on=\"h_geocode\", how=\"left\")\n",
    "           .merge(blk_to_tr.rename(columns={\"geocode\":\"w_geocode\",\"tract_geoid\":\"tract_work\"}),\n",
    "                  on=\"w_geocode\", how=\"left\"))\n",
    "od_tr = od_tr.dropna(subset=[\"tract_home\",\"tract_work\"])\n",
    "od_tr[\"tract_home\"] = od_tr[\"tract_home\"].astype(str).str.zfill(11)\n",
    "od_tr[\"tract_work\"] = od_tr[\"tract_work\"].astype(str).str.zfill(11)\n",
    "\n",
    "# Which tract polygons touch Seattle?\n",
    "tracts_in_sea = set(tracts[tracts.intersects(SEA)][\"GEOID\"])\n",
    "od_tr = od_tr[od_tr[\"tract_work\"].isin(tracts_in_sea)]\n",
    "\n",
    "# Aggregate tract→tract\n",
    "flows_tr = (od_tr.groupby([\"tract_home\",\"tract_work\"], as_index=False)[\"S000\"]\n",
    "                 .sum().rename(columns={\"S000\":\"jobs\"}))\n",
    "\n",
    "# Attach centroids\n",
    "flows_tr = flows_tr.merge(tracts[[\"GEOID\",\"lon\",\"lat\"]],\n",
    "                          left_on=\"tract_home\", right_on=\"GEOID\", how=\"left\") \\\n",
    "                   .rename(columns={\"lon\":\"home_lon\",\"lat\":\"home_lat\"}).drop(columns=[\"GEOID\"])\n",
    "flows_tr = flows_tr.merge(tracts[[\"GEOID\",\"lon\",\"lat\"]],\n",
    "                          left_on=\"tract_work\", right_on=\"GEOID\", how=\"left\") \\\n",
    "                   .rename(columns={\"lon\":\"work_lon\",\"lat\":\"work_lat\"}).drop(columns=[\"GEOID\"])\n",
    "\n",
    "flows_tr = flows_tr.dropna(subset=[\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\"]).copy()\n",
    "flows_tr[\"geo_level\"] = \"tract\"\n",
    "print(\"Tract-level flows:\", len(flows_tr))\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Save outputs (and combined)\n",
    "# -----------------------------\n",
    "cols = [\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\",\"jobs\",\"geo_level\"]\n",
    "\n",
    "flows_block_sea[cols].to_csv(DATA/\"seattle_flows_block_2022.csv\", index=False)\n",
    "flows_bg[cols].to_csv(DATA/\"seattle_flows_blockgroup_2022.csv\", index=False)\n",
    "flows_tr[cols].to_csv(DATA/\"seattle_flows_tract_2022.csv\", index=False)\n",
    "\n",
    "flows_all = pd.concat([flows_block_sea[cols], flows_bg[cols], flows_tr[cols]], ignore_index=True)\n",
    "flows_all.to_csv(DATA/\"seattle_flows_multilevel_2022.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved:\")\n",
    "print(\" - data/seattle_flows_block_2022.csv\")\n",
    "print(\" - data/seattle_flows_blockgroup_2022.csv\")\n",
    "print(\" - data/seattle_flows_tract_2022.csv\")\n",
    "print(\" - data/seattle_flows_multilevel_2022.csv  (all levels in one file)\")\n",
    "\n",
    "# -----------------------------\n",
    "# 9) (Optional) Thin tiny flows for Kepler performance\n",
    "# -----------------------------\n",
    "# Example: keep flows with ≥ 20 jobs\n",
    "# flows_all_thin = flows_all[flows_all[\"jobs\"] >= 20].copy()\n",
    "# flows_all_thin.to_csv(DATA/\"seattle_flows_multilevel_2022_min20.csv\", index=False)\n",
    "# print(\"✅ Also saved: data/seattle_flows_multilevel_2022_min20.csv (jobs >= 20)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c86deb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exists: wa_od_main_JT00_2022.csv.gz\n",
      "✅ Exists: wa_xwalk.csv.gz\n",
      "✅ Exists: tl_2021_53_tract\n",
      "✅ Exists: tl_2021_53_bg\n",
      "✅ Exists: tl_2021_53_place\n",
      "▶ OD rows: 2,905,296\n",
      "▶ Unique work tracts: 1,771 | work BGs: 5,292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lette\\AppData\\Local\\Temp\\ipykernel_618512\\2803506025.py:89: DtypeWarning: Columns (31,35) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  xw = pd.read_csv(xw_fp, dtype={\"tabblk2020\":\"string\"})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Seattle polygon loaded\n",
      "▶ BGs intersecting Seattle: 571\n",
      "▶ Tracts intersecting Seattle: 199\n",
      "▶ Block-level flows (Seattle dest): 544,781\n",
      "▶ OD rows with BG-work in Seattle: 563,744\n",
      "▶ BG→BG pairs: 281,474\n",
      "▶ Block-group flows (final): 281,474\n",
      "▶ OD rows with tract-work in Seattle: 578,207\n",
      "▶ Tract→Tract pairs: 122,950\n",
      "▶ Tract flows (final): 122,950\n",
      "✅ Saved:\n",
      " - data/seattle_flows_block_2022.csv\n",
      " - data/seattle_flows_blockgroup_2022.csv\n",
      " - data/seattle_flows_tract_2022.csv\n",
      " - data/seattle_flows_multilevel_2022.csv  (all levels in one file)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Seattle Commuting Flows (LODES8, WA 2022)\n",
    "# - Block, Block Group, and Tract levels\n",
    "# - GEOIDs derived directly from 15-digit block codes (2020-compatible)\n",
    "# - Destinations filtered to polygons that INTERSECT Seattle\n",
    "# - Centroids computed in projected CRS; diagnostics printed\n",
    "# - Exports: block / blockgroup / tract CSVs + combined CSV\n",
    "# ==========================================================\n",
    "\n",
    "import os, io, zipfile, requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# -----------------------------\n",
    "# Config & helpers\n",
    "# -----------------------------\n",
    "DATA = Path(\"data\"); DATA.mkdir(exist_ok=True)\n",
    "\n",
    "def fetch(url: str, out_path: Path):\n",
    "    if out_path.exists():\n",
    "        print(f\"✅ Exists: {out_path.name}\")\n",
    "        return out_path\n",
    "    print(f\"⬇️  Downloading: {url}\")\n",
    "    r = requests.get(url, timeout=180)\n",
    "    r.raise_for_status()\n",
    "    out_path.write_bytes(r.content)\n",
    "    print(f\"✅ Saved: {out_path.name}\")\n",
    "    return out_path\n",
    "\n",
    "def fetch_unzip(url: str, out_dir: Path) -> Path:\n",
    "    out_dir.mkdir(exist_ok=True, parents=True)\n",
    "    shp = list(out_dir.glob(\"*.shp\"))\n",
    "    if shp:\n",
    "        print(f\"✅ Exists: {out_dir.name}\")\n",
    "        return shp[0]\n",
    "    print(f\"⬇️  Downloading & unzipping: {url}\")\n",
    "    r = requests.get(url, timeout=240)\n",
    "    r.raise_for_status()\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
    "        z.extractall(out_dir)\n",
    "    shp = list(out_dir.glob(\"*.shp\"))\n",
    "    print(f\"✅ Unzipped: {out_dir} ({shp[0].name})\")\n",
    "    return shp[0]\n",
    "\n",
    "def geounion(gdf: gpd.GeoDataFrame):\n",
    "    # union_all for newer versions; fallback to unary_union\n",
    "    return gdf.union_all() if hasattr(gdf, \"union_all\") else gdf.unary_union\n",
    "\n",
    "def centroid_lonlat(gdf: gpd.GeoDataFrame, proj_epsg=3857):\n",
    "    g_proj = gdf.to_crs(proj_epsg)\n",
    "    cents = g_proj.geometry.centroid\n",
    "    cents_ll = gpd.GeoSeries(cents, crs=g_proj.crs).to_crs(4326)\n",
    "    return pd.DataFrame({\"lon\": cents_ll.x.values, \"lat\": cents_ll.y.values})\n",
    "\n",
    "def log(msg): print(f\"▶ {msg}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Download inputs\n",
    "# -----------------------------\n",
    "od_fp  = fetch(\"https://lehd.ces.census.gov/data/lodes/LODES8/wa/od/wa_od_main_JT00_2022.csv.gz\",\n",
    "               DATA / \"wa_od_main_JT00_2022.csv.gz\")\n",
    "xw_fp  = fetch(\"https://lehd.ces.census.gov/data/lodes/LODES8/wa/wa_xwalk.csv.gz\",\n",
    "               DATA / \"wa_xwalk.csv.gz\")\n",
    "tracts_shp  = fetch_unzip(\"https://www2.census.gov/geo/tiger/TIGER2021/TRACT/tl_2021_53_tract.zip\",\n",
    "                          DATA / \"tl_2021_53_tract\")\n",
    "bg_shp      = fetch_unzip(\"https://www2.census.gov/geo/tiger/TIGER2021/BG/tl_2021_53_bg.zip\",\n",
    "                          DATA / \"tl_2021_53_bg\")\n",
    "places_shp  = fetch_unzip(\"https://www2.census.gov/geo/tiger/TIGER2021/PLACE/tl_2021_53_place.zip\",\n",
    "                          DATA / \"tl_2021_53_place\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Load OD and derive GEOIDs from block codes (2020-compatible)\n",
    "# -----------------------------\n",
    "od = pd.read_csv(od_fp, dtype={\"h_geocode\":\"string\",\"w_geocode\":\"string\"})\n",
    "od[\"S000\"] = od[\"S000\"].astype(\"int64\")\n",
    "log(f\"OD rows: {len(od):,}\")\n",
    "\n",
    "# Ensure 15 digits; derive tract (11) and block group (12) from block IDs\n",
    "od[\"h_geocode\"] = od[\"h_geocode\"].str.zfill(15)\n",
    "od[\"w_geocode\"] = od[\"w_geocode\"].str.zfill(15)\n",
    "od[\"tract_home\"] = od[\"h_geocode\"].str[:11]\n",
    "od[\"tract_work\"] = od[\"w_geocode\"].str[:11]\n",
    "od[\"bg_home\"]    = od[\"h_geocode\"].str[:12]\n",
    "od[\"bg_work\"]    = od[\"w_geocode\"].str[:12]\n",
    "log(f\"Unique work tracts: {od['tract_work'].nunique():,} | work BGs: {od['bg_work'].nunique():,}\")\n",
    "\n",
    "# For block-level coordinates we can use crosswalk block coords (lat/lon)\n",
    "xw = pd.read_csv(xw_fp, dtype={\"tabblk2020\":\"string\"})\n",
    "xw[\"tabblk2020\"] = xw[\"tabblk2020\"].str.zfill(15)\n",
    "blk_coords = xw.rename(columns={\"tabblk2020\":\"geocode\",\n",
    "                                \"blklondd\":\"lon\",\n",
    "                                \"blklatdd\":\"lat\"})[[\"geocode\",\"lon\",\"lat\"]]\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Load TIGER shapes (WGS84) + Seattle polygon\n",
    "# -----------------------------\n",
    "tracts = gpd.read_file(tracts_shp).to_crs(4326)\n",
    "tracts[\"GEOID\"] = tracts[\"GEOID\"].astype(str).str.zfill(11)\n",
    "bg = gpd.read_file(bg_shp).to_crs(4326)\n",
    "bg[\"GEOID\"] = bg[\"GEOID\"].astype(str).str.zfill(12)\n",
    "\n",
    "places = gpd.read_file(places_shp).to_crs(4326)\n",
    "seattle = places.query(\"NAME == 'Seattle'\").copy()\n",
    "assert len(seattle) == 1, \"Seattle boundary not found or multiple matches.\"\n",
    "SEA = geounion(seattle)\n",
    "log(\"Seattle polygon loaded\")\n",
    "\n",
    "# Precompute accurate centroids for BG & tracts (projected → back to lon/lat)\n",
    "bg_cent = centroid_lonlat(bg)\n",
    "bg = pd.concat([bg.reset_index(drop=True), bg_cent], axis=1)     # adds lon/lat\n",
    "tracts_cent = centroid_lonlat(tracts)\n",
    "tracts = pd.concat([tracts.reset_index(drop=True), tracts_cent], axis=1)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Build ID sets of destinations that touch Seattle (intersects)\n",
    "# -----------------------------\n",
    "# Do spatial tests in a metric CRS for robustness\n",
    "bg_m = bg.to_crs(3857); tracts_m = tracts.to_crs(3857); seattle_m = seattle.to_crs(3857)\n",
    "SEA_M = geounion(seattle_m)\n",
    "\n",
    "bg_ids_in_sea  = set(bg_m[bg_m.intersects(SEA_M)].index)\n",
    "tr_ids_in_sea  = set(tracts_m[tracts_m.intersects(SEA_M)].index)\n",
    "bg_geoids_in_sea = set(bg.iloc[list(bg_ids_in_sea)][\"GEOID\"]) if bg_ids_in_sea else set()\n",
    "tr_geoids_in_sea = set(tracts.iloc[list(tr_ids_in_sea)][\"GEOID\"]) if tr_ids_in_sea else set()\n",
    "log(f\"BGs intersecting Seattle: {len(bg_geoids_in_sea):,}\")\n",
    "log(f\"Tracts intersecting Seattle: {len(tr_geoids_in_sea):,}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) BLOCK → BLOCK flows (coords from crosswalk; polygon filter)\n",
    "# -----------------------------\n",
    "flows_block = od[[\"h_geocode\",\"w_geocode\",\"S000\"]].rename(columns={\"S000\":\"jobs\"})\n",
    "flows_block = flows_block.merge(blk_coords, left_on=\"h_geocode\", right_on=\"geocode\", how=\"left\") \\\n",
    "                         .rename(columns={\"lon\":\"home_lon\",\"lat\":\"home_lat\"}).drop(columns=[\"geocode\"])\n",
    "flows_block = flows_block.merge(blk_coords, left_on=\"w_geocode\", right_on=\"geocode\", how=\"left\") \\\n",
    "                         .rename(columns={\"lon\":\"work_lon\",\"lat\":\"work_lat\"}).drop(columns=[\"geocode\"])\n",
    "\n",
    "flows_block_gdf = gpd.GeoDataFrame(\n",
    "    flows_block,\n",
    "    geometry=gpd.points_from_xy(flows_block[\"work_lon\"], flows_block[\"work_lat\"]),\n",
    "    crs=4326\n",
    ").to_crs(3857)\n",
    "\n",
    "flows_block_sea = flows_block_gdf[flows_block_gdf.intersects(SEA_M)] \\\n",
    "                      .dropna(subset=[\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\"]).copy()\n",
    "flows_block_sea[\"geo_level\"] = \"block\"\n",
    "log(f\"Block-level flows (Seattle dest): {len(flows_block_sea):,}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) BLOCK GROUP → BLOCK GROUP flows (ID-based dest filter)\n",
    "# -----------------------------\n",
    "# Filter by destination BG GEOIDs that touch Seattle, then aggregate\n",
    "od_bg = od[od[\"bg_work\"].isin(bg_geoids_in_sea)].copy()\n",
    "log(f\"OD rows with BG-work in Seattle: {len(od_bg):,}\")\n",
    "\n",
    "flows_bg = (od_bg.groupby([\"bg_home\",\"bg_work\"], as_index=False)[\"S000\"]\n",
    "                 .sum().rename(columns={\"S000\":\"jobs\"}))\n",
    "log(f\"BG→BG pairs: {len(flows_bg):,}\")\n",
    "\n",
    "# Attach BG centroids\n",
    "bg_pts = bg[[\"GEOID\",\"lon\",\"lat\"]].copy()\n",
    "flows_bg = flows_bg.merge(bg_pts, left_on=\"bg_home\", right_on=\"GEOID\", how=\"left\") \\\n",
    "                   .rename(columns={\"lon\":\"home_lon\",\"lat\":\"home_lat\"}).drop(columns=[\"GEOID\"])\n",
    "flows_bg = flows_bg.merge(bg_pts, left_on=\"bg_work\", right_on=\"GEOID\", how=\"left\") \\\n",
    "                   .rename(columns={\"lon\":\"work_lon\",\"lat\":\"work_lat\"}).drop(columns=[\"GEOID\"])\n",
    "flows_bg = flows_bg.dropna(subset=[\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\"]).copy()\n",
    "flows_bg[\"geo_level\"] = \"blockgroup\"\n",
    "log(f\"Block-group flows (final): {len(flows_bg):,}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7) TRACT → TRACT flows (ID-based dest filter)\n",
    "# -----------------------------\n",
    "od_tr = od[od[\"tract_work\"].isin(tr_geoids_in_sea)].copy()\n",
    "log(f\"OD rows with tract-work in Seattle: {len(od_tr):,}\")\n",
    "\n",
    "flows_tr = (od_tr.groupby([\"tract_home\",\"tract_work\"], as_index=False)[\"S000\"]\n",
    "                 .sum().rename(columns={\"S000\":\"jobs\"}))\n",
    "log(f\"Tract→Tract pairs: {len(flows_tr):,}\")\n",
    "\n",
    "# Attach tract centroids\n",
    "tr_pts = tracts[[\"GEOID\",\"lon\",\"lat\"]].copy()\n",
    "flows_tr = flows_tr.merge(tr_pts, left_on=\"tract_home\", right_on=\"GEOID\", how=\"left\") \\\n",
    "                   .rename(columns={\"lon\":\"home_lon\",\"lat\":\"home_lat\"}).drop(columns=[\"GEOID\"])\n",
    "flows_tr = flows_tr.merge(tr_pts, left_on=\"tract_work\", right_on=\"GEOID\", how=\"left\") \\\n",
    "                   .rename(columns={\"lon\":\"work_lon\",\"lat\":\"work_lat\"}).drop(columns=[\"GEOID\"])\n",
    "flows_tr = flows_tr.dropna(subset=[\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\"]).copy()\n",
    "flows_tr[\"geo_level\"] = \"tract\"\n",
    "log(f\"Tract flows (final): {len(flows_tr):,}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Save outputs (and combined)\n",
    "# -----------------------------\n",
    "cols = [\"home_lon\",\"home_lat\",\"work_lon\",\"work_lat\",\"jobs\",\"geo_level\"]\n",
    "\n",
    "flows_block_sea[cols].to_csv(DATA/\"seattle_flows_block_2022.csv\", index=False)\n",
    "flows_bg[cols].to_csv(DATA/\"seattle_flows_blockgroup_2022.csv\", index=False)\n",
    "flows_tr[cols].to_csv(DATA/\"seattle_flows_tract_2022.csv\", index=False)\n",
    "\n",
    "flows_all = pd.concat([flows_block_sea[cols], flows_bg[cols], flows_tr[cols]], ignore_index=True)\n",
    "flows_all.to_csv(DATA/\"seattle_flows_multilevel_2022.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved:\")\n",
    "print(\" - data/seattle_flows_block_2022.csv\")\n",
    "print(\" - data/seattle_flows_blockgroup_2022.csv\")\n",
    "print(\" - data/seattle_flows_tract_2022.csv\")\n",
    "print(\" - data/seattle_flows_multilevel_2022.csv  (all levels in one file)\")\n",
    "\n",
    "# -----------------------------\n",
    "# 9) (Optional) Thin tiny flows for Kepler performance\n",
    "# -----------------------------\n",
    "# flows_all_min20 = flows_all[flows_all[\"jobs\"] >= 20].copy()\n",
    "# flows_all_min20.to_csv(DATA/\"seattle_flows_multilevel_2022_min20.csv\", index=False)\n",
    "# print(\"✅ Also saved: data/seattle_flows_multilevel_2022_min20.csv (jobs >= 20)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "momepy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
